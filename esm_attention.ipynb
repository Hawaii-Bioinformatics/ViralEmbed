{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "89860edc-2ae3-4914-8fa1-7658ea69da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmForTokenClassification, EsmForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "3ce7c2ef-ddab-4e16-a8bd-93fd0248420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F\n",
    "from typing import Union\n",
    "from functools import lru_cache\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "809753d7-c7ae-4b23-aed4-abb46106007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters : \n",
    "\n",
    "batch_size = 1\n",
    "seq_length = 1024\n",
    "num_attention_heads = 20\n",
    "hidden_size = 1280\n",
    "attention_head_size = int(hidden_size / num_attention_heads)\n",
    "all_head_size = num_attention_heads * attention_head_size\n",
    "\n",
    "position_embedding_type = \"rotary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "169f8de5-ab7a-4ab3-9029-24b95add0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers : \n",
    "\n",
    "query_layer = nn.Linear(hidden_size, all_head_size)\n",
    "key_layer = nn.Linear(hidden_size, all_head_size)\n",
    "value_layer = nn.Linear(hidden_size, all_head_size)\n",
    "\n",
    "dropout_layer = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "593db5d0-a4fa-4fd5-9c3a-4797c4adfdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_layer.in_features, query_layer.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "faed820a-1369-44fa-b1dc-34a885afe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RoPE : \n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:, :, : x.shape[-2], :]\n",
    "    sin = sin[:, :, : x.shape[-2], :]\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary position embeddings based on those in\n",
    "    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n",
    "    matrices which depend on their relative positions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        # Generate and save the inverse frequency buffer (non trainable)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "        inv_freq = inv_freq\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self._seq_len_cached = None\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cos_sin_tables(self, x, seq_dimension=2):\n",
    "        seq_len = x.shape[seq_dimension]\n",
    "\n",
    "        # Reset the tables if the sequence length has changed,\n",
    "        # or if we're on a new device (possibly due to tracing for instance)\n",
    "        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "\n",
    "            self._cos_cached = emb.cos()[None, None, :, :]\n",
    "            self._sin_cached = emb.sin()[None, None, :, :]\n",
    "\n",
    "        return self._cos_cached, self._sin_cached\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
    "\n",
    "        return (\n",
    "            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
    "            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "a21b9f27-1acc-4f6e-bdc2-8346bf2e280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose for attentions :\n",
    "\n",
    "def transpose_for_scores(x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "8d622cac-31e5-4ddf-a5f8-436300b31e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.randn((1, seq_length, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "043756c8-5661-43bd-812f-5d684666a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1280])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "b628fdf7-3ca9-4ea4-ada5-19014a30d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1280])\n",
      "torch.Size([1, 20, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "# Query :\n",
    "\n",
    "mixed_query_layer = query_layer(hidden_states)\n",
    "print(mixed_query_layer.shape)\n",
    "query = transpose_for_scores(mixed_query_layer)\n",
    "print(query.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "6997e43c-86c6-4d7e-88ce-46f3ff3f63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query * attention_head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1fad19ab-b867-4d05-9eac-90b9c673e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key, Value :\n",
    "\n",
    "key = key_layer(hidden_states)\n",
    "key = transpose_for_scores(key)\n",
    "\n",
    "value = value_layer(hidden_states)\n",
    "value = transpose_for_scores(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "34de12d2-99b1-4b1a-923d-72958c65fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding : \n",
    "\n",
    "if position_embedding_type == \"rotary\":\n",
    "    rotary_embeddings = RotaryEmbedding(attention_head_size)\n",
    "    query, key = rotary_embeddings(query, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a26d1b16-83f2-4f81-bc14-e551fcc17a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1024, 64])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "de0cc2b3-1a70-403b-bc3a-7579c0bafcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for full matmul : 0.07536983489990234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1024, 1024])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication for attention scores :\n",
    "\n",
    "start_time = time.time()\n",
    "attention_scores_full = torch.matmul(query, key.transpose(-1, -2))\n",
    "print(f'Elapsed time for full matmul : {time.time()-start_time}')\n",
    "attention_scores_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4da35d0f-c737-4e81-91b0-e4f3a2f8ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask : \n",
    "\n",
    "attention_mask_full = None\n",
    "if attention_mask_full is not None:\n",
    "    # Apply the attention mask is (precomputed for all layers in EsmModel forward() function)\n",
    "    attention_mask_full = attention_mask_full\n",
    "    attention_scores_full = attention_scores_full + attention_mask_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "a2e4f480-1ca5-438d-8657-915eb2b7796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations before Att*V : \n",
    "\n",
    "attention_probs_full = nn.functional.softmax(attention_scores_full, dim=-1)\n",
    "attention_probs_full = dropout_layer(attention_probs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "d7cbf5d1-8224-4090-b5bd-97ae1b2d4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Context layer :\n",
    "\n",
    "context = torch.matmul(attention_probs_full, value)\n",
    "context_layer = context.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)\n",
    "print(context_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "a805dafb-20be-46bd-8672-0c4a8f0a88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "output_attentions = True\n",
    "outputs = (context_layer, attention_probs_full) if output_attentions else (context_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "2007fe35-58fe-45a4-a735-a926437386fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 1280]), torch.Size([1, 20, 1024, 513]))"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9da5b-f965-4181-b027-1440b28cabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025cb79-5f59-4f89-9dce-1d4e99b35875",
   "metadata": {},
   "source": [
    "Longformer Attention (sliding window) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d18e8bf2-2bae-4f26-a928-98e42a8a6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers transformations methods : \n",
    "\n",
    "def _skew(x, direction, padding_value):\n",
    "    '''Convert diagonals into columns (or columns into diagonals depending on `direction`'''\n",
    "    x_padded = F.pad(x, direction, value=padding_value)\n",
    "    x_padded = x_padded.view(*x_padded.size()[:-2], x_padded.size(-1), x_padded.size(-2))\n",
    "    return x_padded\n",
    "\n",
    "def _skew2(x, padding_value):\n",
    "    '''shift every row 1 step to right converting columns into diagonals'''\n",
    "    # X = B x C x M x L\n",
    "    B, C, M, L = x.size()\n",
    "    x = F.pad(x, (0, M + 1), value=padding_value)  # B x C x M x (L+M+1)\n",
    "    x = x.view(B, C, -1)  # B x C x ML+MM+M\n",
    "    x = x[:, :, :-M]  # B x C x ML+MM\n",
    "    x = x.view(B, C, M, M + L)  # B x C, M x L+M\n",
    "    x = x[:, :, :, :-1]\n",
    "    return x\n",
    "\n",
    "def _chunk(x, w):\n",
    "    dim = int(x.size(1) // (w * 2))\n",
    "    x = x.view(x.size(0), dim, int(w * 2), x.size(2))\n",
    "\n",
    "    chunk_size = list(x.size())\n",
    "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "\n",
    "    chunk_stride = list(x.stride())\n",
    "    chunk_stride[1] = chunk_stride[1] // 2\n",
    "    \n",
    "    return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "@lru_cache()\n",
    "def _get_invalid_locations_mask(w: int, d: Union[torch.Tensor,int], autoregressive: bool, device: str):\n",
    "    if isinstance(d, int):\n",
    "        affected_seq_len = w * d\n",
    "        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "        mask = mask[None, :, None, :]\n",
    "    else:\n",
    "        affected_seq_len = w * d.max()\n",
    "        head_masks = []\n",
    "        d_list = d.cpu().numpy().tolist()\n",
    "        for d in d_list:\n",
    "            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "            head_masks.append(one_head_mask)\n",
    "        mask = torch.stack(head_masks, dim=-2)\n",
    "        mask = mask[None, :, :, :]\n",
    "\n",
    "    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool().to(device)\n",
    "    return affected_seq_len, mask.bool().to(device), ending_mask\n",
    "\n",
    "def _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n",
    "    diagonals_list = []\n",
    "    for j in range(-d * w, d, d):\n",
    "        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
    "        diagonal_mask[:-j] = 1\n",
    "        diagonals_list.append(diagonal_mask)\n",
    "    return torch.stack(diagonals_list, dim=-1)\n",
    "\n",
    "def mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: Union[torch.Tensor, int], autoregressive: bool) -> torch.Tensor:\n",
    "    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n",
    "    seq_len = input_tensor.size(1)\n",
    "    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n",
    "    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n",
    "    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n",
    "    if not autoregressive:\n",
    "        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n",
    "        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n",
    "        ending_input.masked_fill_(ending_mask, -float('inf'))\n",
    "\n",
    "def sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n",
    "\n",
    "    bsz, num_heads,seqlen, head_dim = q.size()\n",
    "\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert q.size() == k.size()\n",
    "\n",
    "    chunks_count = seqlen // w - 1\n",
    "\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n",
    "    q = q.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    k = k.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "\n",
    "    chunk_q = _chunk(q, w)\n",
    "    chunk_k = _chunk(k, w)\n",
    "    chunk_attn = torch.einsum('bcxd,bcyd->bcxy', (chunk_q, chunk_k))  # multiply\n",
    "\n",
    "    # convert diagonals into columns\n",
    "    diagonal_chunk_attn = _skew(chunk_attn, direction=(0, 0, 0, 1), padding_value=padding_value)\n",
    "    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n",
    "    \n",
    "    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n",
    "    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n",
    "    # - copying the lower triangle\n",
    "    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n",
    "    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n",
    "\n",
    "    # separate bsz and num_heads dimensions again\n",
    "    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1)\n",
    "    mask_invalid_locations(diagonal_attn, w, 1, True)\n",
    "\n",
    "    return diagonal_attn\n",
    "\n",
    "def sliding_chunks_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n",
    "    \n",
    "    bsz, num_heads,seqlen, head_dim = v.size()\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert prob.size()[:3] == v.size()[:3]\n",
    "    assert prob.size(3) == 2 * w + 1\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size 2w\n",
    "    chunk_prob = prob.reshape(bsz * num_heads, seqlen // w, w, 2 * w + 1)\n",
    "\n",
    "    # group bsz and num_heads dimensions into one\n",
    "    v = v.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "\n",
    "    # pad seqlen with w at the beginning of the sequence and another w at the end\n",
    "    padded_v = F.pad(v, (0, 0, w, w), value=-1)\n",
    "\n",
    "    # chunk padded_v into chunks of size 3w and an overlap of size w\n",
    "    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n",
    "    chunk_v_stride = padded_v.stride()\n",
    "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
    "    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
    "\n",
    "    skewed_prob = _skew2(chunk_prob, padding_value=0)\n",
    "    context = torch.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n",
    "    return context.view(bsz, num_heads, seqlen, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1d14c88-eb17-4a21-90b1-00cb0e941778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers and reshaping for longformer \n",
    "\n",
    "query_states = query_layer(hidden_states)\n",
    "key_states = key_layer(hidden_states)\n",
    "value_states = value_layer(hidden_states)\n",
    "\n",
    "query_states = query_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6b276179-5137-4cf4-afc5-8a74369ed330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "\n",
    "if position_embedding_type == \"rotary\":\n",
    "    rotary_embeddings = RotaryEmbedding(attention_head_size)\n",
    "    query_states, key_states = rotary_embeddings(query_states, key_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "722ea165-71c8-4d62-8ac9-64c37385f391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1024, 513])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention scores\n",
    "\n",
    "window_size = 256\n",
    "\n",
    "attention_scores = sliding_chunks_matmul_qk(query_states, key_states, window_size, padding_value=0)*(1/(seq_length**0.5))\n",
    "attention_scores.shape # torch.Size([batch_size, num_attention_heads, seq_length, w*2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9c3ac88a-98f6-4568-8f18-3de22da73b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask : \n",
    "\n",
    "attention_mask = None\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_scores = attention_scores + attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e8b9812-23bb-4da8-918e-fe404dd7d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention normalization and context layer\n",
    "\n",
    "attention_probs = nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)\n",
    "attention_probs = dropout_layer(attention_probs)\n",
    "\n",
    "context = sliding_chunks_matmul_pv(attention_probs, value_states, window_size)\n",
    "context_layer = context.transpose(1, 2).contiguous()\n",
    "output = context_layer.view(context_layer.size(0),context_layer.size(1),context_layer.size(2)*context_layer.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7b0004c3-138c-429e-a699-8569ffd8abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1280])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # torch.Size([batch_size, seq_length, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484c0d6-aedf-4a05-8a42-8fc8480b509d",
   "metadata": {},
   "source": [
    "Sparse Attention (ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "feb5ee27-0490-457f-ae67-bc1566885135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([116, 184, 134, 57, 169, 128, 191, 45],\n",
       " [0, 116, 300, 434, 491, 660, 788, 979, 1024],\n",
       " [(4, 2),\n",
       "  (7, 1),\n",
       "  (0, 3),\n",
       "  (5, 7),\n",
       "  (3, 7),\n",
       "  (2, 0),\n",
       "  (4, 7),\n",
       "  (5, 0),\n",
       "  (0, 4),\n",
       "  (3, 6)])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input : hidden states + sequence information (proteins + interactions map)\n",
    "\n",
    "# Random proteins information : \n",
    "def generate_list(n, Amin, Amax):\n",
    "    ni_list = []\n",
    "    remaining = n\n",
    "    while remaining > Amin:\n",
    "        ni = random.randint(Amin, min(Amax, remaining))\n",
    "        ni_list.append(ni)\n",
    "        remaining -= ni\n",
    "    ni_list.append(remaining)\n",
    "    return ni_list\n",
    "\n",
    "# Random proteins interactions : \n",
    "def generate_couples(n_couples, n_len):\n",
    "    couples = []\n",
    "    while len(couples) < n_couples :\n",
    "        i = random.randint(0, n_len - 1)\n",
    "        j = random.randint(0, n_len - 1)\n",
    "        if abs(i - j) > 1 and (i, j) not in couples and (j, i) not in couples:\n",
    "            couples.append((i, j))\n",
    "    return couples\n",
    "\n",
    "proteins_list = generate_list(seq_length, Amin = 50, Amax = 200) \n",
    "proteins_cs =  [0]+list(np.cumsum(np.array(proteins_list)))\n",
    "n = len(proteins_list)\n",
    "proteins_interactions = generate_couples(n_couples = 10, n_len = n) # Max number of couples : n * (n - 3) / 2\n",
    "proteins_list, proteins_cs, proteins_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a5212423-62df-4374-a433-5a752790c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_attention_matrix(query, key, proteins_interactions, proteins_cs):\n",
    "    batch_size, num_heads, seq_len, all_head_size = query.shape\n",
    "    start_time = time.time()\n",
    "    attentions = []\n",
    "    sorted_proteins_interactions_x = sorted(proteins_interactions, key=lambda x: x[0])\n",
    "    print(sorted_proteins_interactions_x)\n",
    "    max2 = 0\n",
    "    max3 = 0\n",
    "    for i, j in sorted_proteins_interactions :\n",
    "        # Blocks\n",
    "        query_block = query[:, :, proteins_cs[i]:proteins_cs[i+1], :]\n",
    "        key_block = key[:, :, proteins_cs[j]:proteins_cs[j+1], :]\n",
    "        \n",
    "        # Compute attention matrix for the 2 blocks\n",
    "        attention_block = torch.matmul(query_block, key_block.transpose(-1, -2)).squeeze()\n",
    "        attentions.append(attention_block)\n",
    "        \n",
    "        if attention_block.shape[-2] > max2 :\n",
    "            max2 = attention_block.shape[-2]\n",
    "        if attention_block.shape[-1] > max3 :\n",
    "            max3 = attention_block.shape[-1]\n",
    "\n",
    "    # Padding for bsr storage :\n",
    "    padded_attentions = []\n",
    "    for block in attentions :\n",
    "        s = block.shape\n",
    "        padding = (0, max3 - s[-1], 0, max2 - s[-2])\n",
    "        padded_attention_block = F.pad(block, padding, \"constant\", 0)\n",
    "        padded_attentions.append(padded_attention_block)\n",
    "        \n",
    "    print(f'Time for attention matrix computation : {time.time()-start_time}')\n",
    "    \n",
    "    # Create block sparse matrix with torch.sparse_bsr_tensor\n",
    "    # columns and rows : \n",
    "    columns = [x[1] for x in sorted_proteins_interactions_x]\n",
    "    # values :\n",
    "    concatenated_attentions = torch.stack(padded_attentions, dim=1)\n",
    "    print(concatenated_attentions.shape)\n",
    "    sparse_matrix = torch.sparse_bsr_tensor(indices_tensor, values_tensor, (seq_length*all_head_size, seq_length*all_head_size))\n",
    "\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d6ecd0b8-57cb-49ff-8b61-e0cbe9d37d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (0, 4), (2, 0), (3, 7), (3, 6), (4, 2), (4, 7), (5, 7), (5, 0), (7, 1)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sorted_proteins_interactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[326], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attentions_scores_sparse \u001b[38;5;241m=\u001b[39m \u001b[43msparse_attention_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproteins_interactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproteins_cs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[325], line 9\u001b[0m, in \u001b[0;36msparse_attention_matrix\u001b[0;34m(query, key, proteins_interactions, proteins_cs)\u001b[0m\n\u001b[1;32m      7\u001b[0m max2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m max3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[43msorted_proteins_interactions\u001b[49m :\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Blocks\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     query_block \u001b[38;5;241m=\u001b[39m query[:, :, proteins_cs[i]:proteins_cs[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], :]\n\u001b[1;32m     12\u001b[0m     key_block \u001b[38;5;241m=\u001b[39m key[:, :, proteins_cs[j]:proteins_cs[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_proteins_interactions' is not defined"
     ]
    }
   ],
   "source": [
    "attentions_scores_sparse = sparse_attention_matrix(query, key, proteins_interactions, proteins_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315a92f-0c87-4394-af82-d0062aec8210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d9162464-3c91-4a0e-8e94-5dcd8e483dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([0, 2, 4, 8]),\n",
       "       col_indices=tensor([0, 1, 2, 0, 4, 5, 6, 7]),\n",
       "       values=tensor([[[1, 2],\n",
       "                       [3, 4]],\n",
       "\n",
       "                      [[5, 6],\n",
       "                       [7, 8]],\n",
       "\n",
       "                      [[1, 2],\n",
       "                       [3, 4]],\n",
       "\n",
       "                      [[5, 6],\n",
       "                       [7, 8]],\n",
       "\n",
       "                      [[1, 2],\n",
       "                       [3, 4]],\n",
       "\n",
       "                      [[5, 6],\n",
       "                       [7, 8]],\n",
       "\n",
       "                      [[1, 2],\n",
       "                       [3, 4]],\n",
       "\n",
       "                      [[5, 6],\n",
       "                       [7, 8]]]), size=(6, 16), nnz=8, layout=torch.sparse_bsr)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crow_indices = [0, 2, 4, 8]\n",
    "col_indices = [0, 1, 2, 0, 4, 5 , 6, 7]\n",
    "values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[1, 2], [3, 4]], [[5, 6], [7, 8]], [[1, 2], [3, 4]], [[5, 6], [7, 8]],[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\n",
    "bsr = torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64),torch.tensor(col_indices, dtype=torch.int64),torch.tensor(values), dtype=torch.int64)\n",
    "bsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f5cf8297-8a0f-4551-a7c6-ca99e1f3edc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 4, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [5, 6, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [7, 8, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 5, 6, 1, 2, 5, 6],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 7, 8, 3, 4, 7, 8]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsr.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6eed454b-665f-4c6f-b9be-f439930be8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(values[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "305ea77b-3cc1-4be0-82bf-98f426b0bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [0, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supposons que vous ayez des valeurs avec des indices inégaux\n",
    "indices = torch.tensor([[0, 1, 2, 2], [0, 1, 0, 1]])  # Les indices de chaque élément non-nul\n",
    "values = torch.tensor([1, 2, 3, 4])  # Les valeurs correspondantes\n",
    "size = (3, 2)  # La taille totale de la matrice\n",
    "\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n",
    "print(sparse_tensor.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae166ba3-7828-466d-9dbf-027c899d8be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
