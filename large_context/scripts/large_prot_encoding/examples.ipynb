{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from model import SparseForTokenClassification\n",
    "from typing import Tuple, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196eff1",
   "metadata": {},
   "source": [
    "### Extracting Embedding for Both Protein Sequences and for Genome\n",
    "#####  Extracting protien for a single protein \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d03e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5493a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usign a short 10 amino acid sequence as a proof of concept.\n",
    "\n",
    "data  = [('seq_1', 'MLKKLSVFLI')]\n",
    "\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c4639",
   "metadata": {},
   "source": [
    "##### Recreate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ead1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '3C' # or '5B'\n",
    "checkpoint = torch.load(f'./models/{version}/config_and_model.pth', map_location='cpu', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "config = checkpoint['config']\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "sparse_model = SparseForTokenClassification(config=config)\n",
    "sparse_model.load_state_dict(model_state_dict)\n",
    "sparse_model = sparse_model.to(device)\n",
    "sparse_model = sparse_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_attention(attentions: Tuple[torch.Tensor, ...], weight_scheme: List[float]) -> torch.Tensor:\n",
    "    x,y,n1,n2 = len(attentions), attentions[0][0].shape[0], attentions[0][0].shape[1], attentions[0][0].shape[2]\n",
    "    assert len(weight_scheme) == x * y\n",
    "    \n",
    "    weighted_sum = torch.zeros_like(attentions[0][0][0])\n",
    "\n",
    "    stacked_attentions = torch.stack(attentions, dim=0)\n",
    "    weights = torch.tensor(weight_scheme, dtype=stacked_attentions.dtype, device=stacked_attentions.device)\n",
    "\n",
    "    flattened_attentions = stacked_attentions.view(x * y, n1, n2)\n",
    "    weighted_sum = torch.sum(flattened_attentions * weights[:, None, None], dim=0)\n",
    "    \n",
    "    return weighted_sum\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbbace3",
   "metadata": {},
   "source": [
    "### We can extract the sequence embeddings using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits[0,1:-1:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[output.attentions[i].size() for i in range(len(output.attentions))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46373c",
   "metadata": {},
   "source": [
    "# Thibaut \n",
    "\n",
    "Could you include the code here that combines all the attention heads and layers into a single 10x10 matrix? It should be in a single function. For now, we will provide equal weights to each matrix, but the function should be able to take a different weighting scheme for each layer. See scaffold below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ae5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_combined_attention(attentions: Tuple[torch.Tensor, ...], weight_scheme: List[float]) -> torch.Tensor:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157784ee",
   "metadata": {},
   "source": [
    "### Computing the Complete Genome Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b076df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = [('seq_1', 'MLKKLSVFLI'), ('seq_2', 'MLSVFLI'), ('seq_3', 'LKKLSVFLI'), ('seq_4', 'MLKKLSVIMMMKV')]\n",
    "\n",
    "# concat all the sequences in data\n",
    "prots_concatenated = \"\".join([x[1] for x in data])\n",
    "prots_lengths = [len(x[1]) for x in data]\n",
    "genome = [(\"some_genome\", prots_concatenated)]\n",
    "genome , prots_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genome[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fafe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels, batch_strs, batch_tokens = batch_converter(genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sparse_model(input_ids=batch_tokens, \n",
    "                      output_attentions = True, \n",
    "                      proteins_sizes = torch.tensor(prots_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539877fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8117ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output['attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['attentions'][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prots_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIBAUT: my understanding is that two_step_selection= True determines whether to return all pairwise scores  or  not.\n",
    "# Adding here, causes an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sparse_model(input_ids=batch_tokens, \n",
    "                      output_attentions = True, \n",
    "                      proteins_sizes = torch.tensor(prots_lengths),  two_step_selection= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096206d",
   "metadata": {},
   "source": [
    "### Ranking the pairwise interactions\n",
    "\n",
    "1. I know that we need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bec958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 assembled fragments\n",
      "Number of sequences to process : 0\n",
      "10 cpus availables\n",
      "0 fragments processed\n"
     ]
    }
   ],
   "source": [
    "from pair_ranking_script import get_pairs_dataset\n",
    "# I am going to run the code below step by step we can use exactly what is needed\n",
    "# output = get_pairs_dataset(\"../data/LR699048.fa\", \"./data/LR699048.pt\", \"./data/LR699048.txt\", plot_fragment_heatmap = False)\n",
    "# # generates the folowing\n",
    "# all_pairs: attention between all pairs of proteins \n",
    "# all_nc_pairs:  attention between all pairs of non-consecutive proteins\n",
    "# global_length: total number of all pairwise comparisons ()\n",
    "# non_consecutive_length: total number of all non-consecutive pairwise comparisons\n",
    "# database_dic: same as all_pairs but in dictionary format where key is genome name\n",
    "# all_top_pairs_labeled: same as all_pairs but with protein labels instead of protein indexes. Uses the labels_file to get the labels\n",
    "# all_top_pairs_labeled_nc: same as all_nc_pairs but with protein labels instead of protein indexes. Uses the labels_file to get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5ffcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pair_ranking_script import load_fasta_as_tuples\n",
    "data_assembled = load_fasta_as_tuples(\"../data/LR699048.fa\")\n",
    "    global_length = []\n",
    "    non_consecutive_length = []\n",
    "    all_length_dict = []\n",
    "    j = 0\n",
    "    all_top_pairs = []\n",
    "    all_top_pairs_nc = []\n",
    "    all_top_pairs_labeled = []\n",
    "    all_top_pairs_labeled_nc = []\n",
    "    database_dic = {}\n",
    "    mlp = []\n",
    "    genomes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIBAUT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
