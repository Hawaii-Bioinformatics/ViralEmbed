{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89860edc-2ae3-4914-8fa1-7658ea69da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmForTokenClassification, EsmForMaskedLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce7c2ef-ddab-4e16-a8bd-93fd0248420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F\n",
    "from typing import Union\n",
    "from functools import lru_cache\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809753d7-c7ae-4b23-aed4-abb46106007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters : \n",
    "\n",
    "batch_size = 1\n",
    "seq_length = 8192\n",
    "num_attention_heads = 20\n",
    "hidden_size = 1280\n",
    "attention_head_size = int(hidden_size / num_attention_heads)\n",
    "all_head_size = num_attention_heads * attention_head_size\n",
    "\n",
    "position_embedding_type = \"rotary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169f8de5-ab7a-4ab3-9029-24b95add0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize layers : \n",
    "\n",
    "query_layer = nn.Linear(hidden_size, all_head_size)\n",
    "key_layer = nn.Linear(hidden_size, all_head_size)\n",
    "value_layer = nn.Linear(hidden_size, all_head_size)\n",
    "\n",
    "dropout_layer = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593db5d0-a4fa-4fd5-9c3a-4797c4adfdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280, 1280)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_layer.in_features, query_layer.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faed820a-1369-44fa-b1dc-34a885afe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RoPE : \n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    cos = cos[:, :, : x.shape[-2], :]\n",
    "    sin = sin[:, :, : x.shape[-2], :]\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary position embeddings based on those in\n",
    "    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n",
    "    matrices which depend on their relative positions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        # Generate and save the inverse frequency buffer (non trainable)\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
    "        inv_freq = inv_freq\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "        self._seq_len_cached = None\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def _update_cos_sin_tables(self, x, seq_dimension=2):\n",
    "        seq_len = x.shape[seq_dimension]\n",
    "\n",
    "        # Reset the tables if the sequence length has changed,\n",
    "        # or if we're on a new device (possibly due to tracing for instance)\n",
    "        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "\n",
    "            self._cos_cached = emb.cos()[None, None, :, :]\n",
    "            self._sin_cached = emb.sin()[None, None, :, :]\n",
    "\n",
    "        return self._cos_cached, self._sin_cached\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
    "\n",
    "        return (\n",
    "            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
    "            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21b9f27-1acc-4f6e-bdc2-8346bf2e280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose for attentions :\n",
    "\n",
    "def transpose_for_scores(x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d622cac-31e5-4ddf-a5f8-436300b31e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.randn((1, seq_length, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043756c8-5661-43bd-812f-5d684666a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8192, 1280])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b628fdf7-3ca9-4ea4-ada5-19014a30d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8192, 1280])\n",
      "torch.Size([1, 20, 8192, 64])\n"
     ]
    }
   ],
   "source": [
    "# Query :\n",
    "\n",
    "mixed_query_layer = query_layer(hidden_states)\n",
    "print(mixed_query_layer.shape)\n",
    "query = transpose_for_scores(mixed_query_layer)\n",
    "print(query.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6997e43c-86c6-4d7e-88ce-46f3ff3f63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query * attention_head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fad19ab-b867-4d05-9eac-90b9c673e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key, Value :\n",
    "\n",
    "key = key_layer(hidden_states)\n",
    "key = transpose_for_scores(key)\n",
    "\n",
    "value = value_layer(hidden_states)\n",
    "value = transpose_for_scores(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34de12d2-99b1-4b1a-923d-72958c65fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding : \n",
    "\n",
    "if position_embedding_type == \"rotary\":\n",
    "    rotary_embeddings = RotaryEmbedding(attention_head_size)\n",
    "    query, key = rotary_embeddings(query, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a26d1b16-83f2-4f81-bc14-e551fcc17a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 8192, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0cc2b3-1a70-403b-bc3a-7579c0bafcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for full matmul : 5.058045864105225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 8192, 8192])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication for attention scores :\n",
    "\n",
    "start_time = time.time()\n",
    "attention_scores_full = torch.matmul(query, key.transpose(-1, -2))\n",
    "print(f'Elapsed time for full matmul : {time.time()-start_time}')\n",
    "attention_scores_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da35d0f-c737-4e81-91b0-e4f3a2f8ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask : \n",
    "\n",
    "attention_mask_full = None\n",
    "if attention_mask_full is not None:\n",
    "    # Apply the attention mask is (precomputed for all layers in EsmModel forward() function)\n",
    "    attention_mask_full = attention_mask_full\n",
    "    attention_scores_full = attention_scores_full + attention_mask_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e4f480-1ca5-438d-8657-915eb2b7796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations before Att*V : \n",
    "\n",
    "attention_probs_full = nn.functional.softmax(attention_scores_full, dim=-1)\n",
    "attention_probs_full = dropout_layer(attention_probs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7cbf5d1-8224-4090-b5bd-97ae1b2d4af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8192, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Context layer :\n",
    "\n",
    "context = torch.matmul(attention_probs_full, value)\n",
    "context_layer = context.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)\n",
    "print(context_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a805dafb-20be-46bd-8672-0c4a8f0a88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "output_attentions = True\n",
    "outputs = (context_layer, attention_probs_full) if output_attentions else (context_layer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2007fe35-58fe-45a4-a735-a926437386fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8192, 1280]), torch.Size([1, 20, 8192, 8192]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape, outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3c9da5b-f965-4181-b027-1440b28cabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025cb79-5f59-4f89-9dce-1d4e99b35875",
   "metadata": {},
   "source": [
    "Longformer Attention (sliding window) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18e8bf2-2bae-4f26-a928-98e42a8a6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers transformations methods : \n",
    "\n",
    "def _skew(x, direction, padding_value):\n",
    "    '''Convert diagonals into columns (or columns into diagonals depending on `direction`'''\n",
    "    x_padded = F.pad(x, direction, value=padding_value)\n",
    "    x_padded = x_padded.view(*x_padded.size()[:-2], x_padded.size(-1), x_padded.size(-2))\n",
    "    return x_padded\n",
    "\n",
    "def _skew2(x, padding_value):\n",
    "    '''shift every row 1 step to right converting columns into diagonals'''\n",
    "    # X = B x C x M x L\n",
    "    B, C, M, L = x.size()\n",
    "    x = F.pad(x, (0, M + 1), value=padding_value)  # B x C x M x (L+M+1)\n",
    "    x = x.view(B, C, -1)  # B x C x ML+MM+M\n",
    "    x = x[:, :, :-M]  # B x C x ML+MM\n",
    "    x = x.view(B, C, M, M + L)  # B x C, M x L+M\n",
    "    x = x[:, :, :, :-1]\n",
    "    return x\n",
    "\n",
    "def _chunk(x, w):\n",
    "    dim = int(x.size(1) // (w * 2))\n",
    "    x = x.view(x.size(0), dim, int(w * 2), x.size(2))\n",
    "\n",
    "    chunk_size = list(x.size())\n",
    "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "\n",
    "    chunk_stride = list(x.stride())\n",
    "    chunk_stride[1] = chunk_stride[1] // 2\n",
    "    \n",
    "    return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "@lru_cache()\n",
    "def _get_invalid_locations_mask(w: int, d: Union[torch.Tensor,int], autoregressive: bool, device: str):\n",
    "    if isinstance(d, int):\n",
    "        affected_seq_len = w * d\n",
    "        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "        mask = mask[None, :, None, :]\n",
    "    else:\n",
    "        affected_seq_len = w * d.max()\n",
    "        head_masks = []\n",
    "        d_list = d.cpu().numpy().tolist()\n",
    "        for d in d_list:\n",
    "            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "            head_masks.append(one_head_mask)\n",
    "        mask = torch.stack(head_masks, dim=-2)\n",
    "        mask = mask[None, :, :, :]\n",
    "\n",
    "    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool().to(device)\n",
    "    return affected_seq_len, mask.bool().to(device), ending_mask\n",
    "\n",
    "def _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n",
    "    diagonals_list = []\n",
    "    for j in range(-d * w, d, d):\n",
    "        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
    "        diagonal_mask[:-j] = 1\n",
    "        diagonals_list.append(diagonal_mask)\n",
    "    return torch.stack(diagonals_list, dim=-1)\n",
    "\n",
    "def mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: Union[torch.Tensor, int], autoregressive: bool) -> torch.Tensor:\n",
    "    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n",
    "    seq_len = input_tensor.size(1)\n",
    "    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n",
    "    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n",
    "    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n",
    "    if not autoregressive:\n",
    "        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n",
    "        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n",
    "        ending_input.masked_fill_(ending_mask, -float('inf'))\n",
    "\n",
    "def sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n",
    "\n",
    "    bsz, num_heads,seqlen, head_dim = q.size()\n",
    "\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert q.size() == k.size()\n",
    "\n",
    "    chunks_count = seqlen // w - 1\n",
    "\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n",
    "    q = q.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    k = k.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "\n",
    "    chunk_q = _chunk(q, w)\n",
    "    chunk_k = _chunk(k, w)\n",
    "    chunk_attn = torch.einsum('bcxd,bcyd->bcxy', (chunk_q, chunk_k))  # multiply\n",
    "\n",
    "    # convert diagonals into columns\n",
    "    diagonal_chunk_attn = _skew(chunk_attn, direction=(0, 0, 0, 1), padding_value=padding_value)\n",
    "    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n",
    "    \n",
    "    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n",
    "    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n",
    "    # - copying the lower triangle\n",
    "    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n",
    "    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n",
    "\n",
    "    # separate bsz and num_heads dimensions again\n",
    "    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1)\n",
    "    mask_invalid_locations(diagonal_attn, w, 1, True)\n",
    "\n",
    "    return diagonal_attn\n",
    "\n",
    "def sliding_chunks_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n",
    "    \n",
    "    bsz, num_heads,seqlen, head_dim = v.size()\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert prob.size()[:3] == v.size()[:3]\n",
    "    assert prob.size(3) == 2 * w + 1\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size 2w\n",
    "    chunk_prob = prob.reshape(bsz * num_heads, seqlen // w, w, 2 * w + 1)\n",
    "\n",
    "    # group bsz and num_heads dimensions into one\n",
    "    v = v.reshape(bsz * num_heads, seqlen, head_dim)\n",
    "\n",
    "    # pad seqlen with w at the beginning of the sequence and another w at the end\n",
    "    padded_v = F.pad(v, (0, 0, w, w), value=-1)\n",
    "\n",
    "    # chunk padded_v into chunks of size 3w and an overlap of size w\n",
    "    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n",
    "    chunk_v_stride = padded_v.stride()\n",
    "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
    "    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
    "\n",
    "    skewed_prob = _skew2(chunk_prob, padding_value=0)\n",
    "    context = torch.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n",
    "    return context.view(bsz, num_heads, seqlen, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d14c88-eb17-4a21-90b1-00cb0e941778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers and reshaping for longformer \n",
    "\n",
    "query_states = query_layer(hidden_states)\n",
    "key_states = key_layer(hidden_states)\n",
    "value_states = value_layer(hidden_states)\n",
    "\n",
    "query_states = query_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n",
    "key_states = key_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n",
    "value_states = value_states.view(batch_size, seq_length, num_attention_heads, attention_head_size).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b276179-5137-4cf4-afc5-8a74369ed330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPE\n",
    "\n",
    "if position_embedding_type == \"rotary\":\n",
    "    rotary_embeddings = RotaryEmbedding(attention_head_size)\n",
    "    query_states, key_states = rotary_embeddings(query_states, key_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "722ea165-71c8-4d62-8ac9-64c37385f391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 8192, 513])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention scores\n",
    "\n",
    "window_size = 256\n",
    "\n",
    "attention_scores = sliding_chunks_matmul_qk(query_states, key_states, window_size, padding_value=0)*(1/(seq_length**0.5))\n",
    "attention_scores.shape # torch.Size([batch_size, num_attention_heads, seq_length, w*2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c3ac88a-98f6-4568-8f18-3de22da73b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mask : \n",
    "\n",
    "attention_mask = None\n",
    "\n",
    "if attention_mask is not None:\n",
    "    attention_scores = attention_scores + attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e8b9812-23bb-4da8-918e-fe404dd7d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention normalization and context layer\n",
    "\n",
    "attention_probs = nn.functional.softmax(attention_scores, dim=-1, dtype=torch.float32)\n",
    "attention_probs = dropout_layer(attention_probs)\n",
    "\n",
    "context = sliding_chunks_matmul_pv(attention_probs, value_states, window_size)\n",
    "context_layer = context.transpose(1, 2).contiguous()\n",
    "output = context_layer.view(context_layer.size(0),context_layer.size(1),context_layer.size(2)*context_layer.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b0004c3-138c-429e-a699-8569ffd8abeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8192, 1280])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # torch.Size([batch_size, seq_length, hidden_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9484c0d6-aedf-4a05-8a42-8fc8480b509d",
   "metadata": {},
   "source": [
    "Sparse Attention (ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb5ee27-0490-457f-ae67-bc1566885135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "330\n",
      "2079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input : hidden states + sequence information (proteins + interactions map)\n",
    "\n",
    "# Random proteins information : \n",
    "def generate_list(n, Amin, Amax):\n",
    "    ni_list = []\n",
    "    remaining = n\n",
    "    while remaining > Amin:\n",
    "        ni = random.randint(Amin, min(Amax, remaining))\n",
    "        ni_list.append(ni)\n",
    "        remaining -= ni\n",
    "    ni_list.append(remaining)\n",
    "    return ni_list\n",
    "\n",
    "# Random proteins interactions : \n",
    "def generate_couples(n_couples, n_len):\n",
    "    couples = []\n",
    "    while len(couples) < n_couples :\n",
    "        i = random.randint(0, n_len - 1)\n",
    "        j = random.randint(0, n_len - 1)\n",
    "        if abs(i - j) > 1 and (i, j) not in couples and (j, i) not in couples:\n",
    "            couples.append((i, j))\n",
    "    return couples\n",
    "\n",
    "proteins_list = generate_list(seq_length, Amin = 50, Amax = 200) \n",
    "proteins_cs =  [0]+list(np.cumsum(np.array(proteins_list)))\n",
    "n = len(proteins_list)\n",
    "print(n)\n",
    "proteins_interactions = generate_couples(n_couples = 5 * n, n_len = n) # Max number of couples : n * (n - 3) / 2\n",
    "print(len(proteins_interactions)), print(int(n * (n - 3) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b27b35f-26ad-41c3-976b-df5525d0e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_to_crows(rows, n): \n",
    "    rows = np.array(rows)\n",
    "    counts = np.bincount(rows, minlength=n+1)\n",
    "    \n",
    "    # Si le tableau résultant est plus court que n+1, on ajoute des zéros à la fin\n",
    "    if counts.size < n+1:\n",
    "        counts = np.pad(counts, (0, n+1-counts.size), constant_values=0)\n",
    "    counts_cs = np.cumsum(counts)\n",
    "    return counts_cs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5212423-62df-4374-a433-5a752790c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_attention_matrix(query, key, proteins_interactions, proteins_cs, proteins_list):\n",
    "    tt = time.time()\n",
    "    batch_size, num_heads, seq_len, all_head_size = query.shape\n",
    "    start_time = time.time()\n",
    "    attentions = []\n",
    "    block_positions = []\n",
    "    block_sizes = []\n",
    "    sorted_proteins_interactions = sorted(proteins_interactions, key=lambda x: x[0])\n",
    "    max2 = 0\n",
    "    max3 = 0\n",
    "    tfor = time.time()\n",
    "    spm = []\n",
    "    for i, j in sorted_proteins_interactions :\n",
    "        \n",
    "        # Blocks\n",
    "        query_block = query[:, :, proteins_cs[i]:proteins_cs[i+1], :]\n",
    "        key_block = key[:, :, proteins_cs[j]:proteins_cs[j+1], :]\n",
    "        \n",
    "        # Compute attention matrix for the 2 blocks\n",
    "        attention_block = torch.matmul(query_block, key_block.transpose(-1, -2)).squeeze()\n",
    "        attentions.append(attention_block) \n",
    "\n",
    "        max2 = max(max2, attention_block.shape[-2])\n",
    "        max3 = max(max3, attention_block.shape[-1])\n",
    "\n",
    "    at = time.time()-start_time\n",
    "    \n",
    "    # Padding for bsr storage :\n",
    "    padded_attentions = []\n",
    "    for attention_block in attentions:\n",
    "        s = attention_block.shape\n",
    "        padded_attention_block = F.pad(attention_block, (0, max3 - s[-1], 0, max2 - s[-2]), \"constant\", 0)\n",
    "        padded_attentions.append(padded_attention_block)\n",
    "        \n",
    "    # Create block sparse matrix with torch.sparse_bsr_tensor\n",
    "    \n",
    "    # columns and rows : \n",
    "    t2 = time.time()\n",
    "    col_indices = [x[1] for x in sorted_proteins_interactions]\n",
    "    rows = [x[0] for x in sorted_proteins_interactions]\n",
    "    crow_indices = [0] + rows_to_crows(rows, len(proteins_cs) - 1)[:-1]\n",
    "    crow_tensor = torch.stack([torch.tensor(crow_indices)] * 20)\n",
    "    col_tensor = torch.stack([torch.tensor(col_indices)] * 20)\n",
    "\n",
    "    # values :    \n",
    "    concatenated_attentions = torch.stack(padded_attentions, dim=1)\n",
    "\n",
    "    #sparse_matrix = create_sparse_coo_with_variable_blocks(attentions, block_positions, seq_len, seq_len)\n",
    "    sparse_matrix = torch.sparse_bsr_tensor(crow_tensor, col_tensor, concatenated_attentions, size = [num_heads, len(proteins_list)*max2, len(proteins_list)*max3])\n",
    "    mt = time.time() - t2\n",
    "    ft = time.time()-tt\n",
    "    print(at, mt, ft)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6ecd0b8-57cb-49ff-8b61-e0cbe9d37d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4056520462036133 0.8604848384857178 1.9431049823760986\n"
     ]
    }
   ],
   "source": [
    "attentions_scores_sparse = sparse_attention_matrix(query, key, proteins_interactions, proteins_cs, proteins_list)\n",
    "#print(attentions_scores_sparse.size())\n",
    "#print(attentions_scores_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315a92f-0c87-4394-af82-d0062aec8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_scores = attentions_scores_sparse.to_dense()\n",
    "dense_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54242aae-51b6-4da6-a19a-e9793d3f62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_elements = torch.prod(torch.tensor(attentions_scores_sparse.shape)).item()  # Nombre total d'éléments dans le tensor dense\n",
    "non_zero_count = attentions_scores_sparse._nnz()*max(proteins_list)*max(proteins_list)*20  # Nombre d'éléments non nuls stockés dans le sparse tensor\n",
    "zero_count = total_elements - non_zero_count  # Nombre d'éléments nuls\n",
    "zero_percentage = (zero_count / total_elements) * 100  # Pourcentage de zéros\n",
    "\n",
    "print(f\"Total elements: {total_elements}\")\n",
    "print(f\"Non-zero elements: {non_zero_count}\")\n",
    "print(f\"Zero count: {zero_count}\")\n",
    "print(f\"Percentage of zero elements: {zero_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "037dca50-80dd-4c19-a1b3-a35f1a48839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_attention_matrix_subblock(query, key, proteins_interactions, proteins_cs, proteins_list, block_size):\n",
    "    batch_size, num_heads, seq_len, all_head_size = query.shape\n",
    "    start_time = time.time()\n",
    "    sub_attentions = []\n",
    "    block_positions = []\n",
    "    block_sizes = []\n",
    "    sorted_proteins_interactions = sorted(proteins_interactions, key=lambda x: x[0])\n",
    "    max2 = 0\n",
    "    max3 = 0\n",
    "\n",
    "    num_blocks = [0]+[int(length//block_size) for length in proteins_list]\n",
    "    num_blocks_cs = np.cumsum(np.array(num_blocks)).tolist()\n",
    "\n",
    "    # Iterating over pairs to compute block-wise attention\n",
    "    for i, j in sorted_proteins_interactions:\n",
    "        #print(i, j)\n",
    "        query_block = query[:, :, proteins_cs[i]:proteins_cs[i+1], :]\n",
    "        key_block = key[:, :, proteins_cs[j]:proteins_cs[j+1], :]\n",
    "        attention_block = torch.matmul(query_block, key_block.transpose(-1, -2)).squeeze()\n",
    "\n",
    "        # Cut the attention blocks into smaller block_size x block_size blocks\n",
    "        num_rows, num_cols = attention_block.shape[-2], attention_block.shape[-1]\n",
    "        for start_row in range(0, num_rows, block_size):\n",
    "            for start_col in range(0, num_cols, block_size):\n",
    "                if start_row + block_size <= num_rows and start_col + block_size <= num_cols:\n",
    "                    sub_block = attention_block[:, start_row:start_row+block_size, start_col:start_col+block_size]\n",
    "                    sub_attentions.append(sub_block)\n",
    "                    #print(start_row, start_col)\n",
    "                    block_positions.append((num_blocks_cs[i] + start_row // 50, num_blocks_cs[j] + start_col // 50))\n",
    "\n",
    "                    \n",
    "    assert len(sub_attentions) == len(block_positions)\n",
    "    print(f'Time for attention matrix computation: {time.time() - start_time}')\n",
    "    \n",
    "    # Create the BSR matrix\n",
    "    col_indices = [x[1] for x in block_positions]\n",
    "    rows = [x[0] for x in block_positions]\n",
    "    crow_indices = [0] + rows_to_crows(rows, len(proteins_cs) - 1)[:-1]\n",
    "    \n",
    "    crow_tensor = torch.stack([torch.tensor(crow_indices)] * 20)\n",
    "    col_tensor = torch.stack([torch.tensor(col_indices)] * 20)\n",
    "    \n",
    "    value_tensor = torch.stack(sub_attentions, dim=1)\n",
    "    sparse_matrix = torch.sparse_bsr_tensor(crow_tensor, col_tensor, value_tensor, size=[num_heads, sum(num_blocks)*block_size, sum(num_blocks)*block_size])\n",
    "\n",
    "    print(f'Time for sparse matrix creation: {time.time() - start_time}')\n",
    "\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4160e96e-1187-4997-a167-b5ebf9d6f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for attention matrix computation: 0.1936037540435791\n",
      "Time for sparse matrix creation: 0.22324180603027344\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix_subblock = sparse_attention_matrix_subblock(query, key, proteins_interactions, proteins_cs, proteins_list, block_size=50)\n",
    "# sparse_matrix_subblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "cc78b816-108c-4b99-9faf-316c38f12df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3300, 3300])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_scores_sub = sparse_matrix_subblock.to_dense()\n",
    "dense_scores_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f504ff2d-3916-473b-b7fe-0ec54c0f4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elements: 217800000\n",
      "Non-zero elements: 30950000\n",
      "Zero count: 186850000\n",
      "Percentage of zero elements: 85.79%\n"
     ]
    }
   ],
   "source": [
    "block_size = 50\n",
    "total_elements = torch.prod(torch.tensor(sparse_matrix_subblock.shape)).item()  # Nombre total d'éléments dans le tensor dense\n",
    "non_zero_count = sparse_matrix_subblock._nnz()*block_size*block_size*20  # Nombre d'éléments non nuls stockés dans le sparse tensor\n",
    "zero_count = total_elements - non_zero_count  # Nombre d'éléments nuls\n",
    "zero_percentage = (zero_count / total_elements) * 100  # Pourcentage de zéros\n",
    "\n",
    "print(f\"Total elements: {total_elements}\")\n",
    "print(f\"Non-zero elements: {non_zero_count}\")\n",
    "print(f\"Zero count: {zero_count}\")\n",
    "print(f\"Percentage of zero elements: {zero_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a507557-c789-4073-8357-deaf73e7ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ae166ba3-7828-466d-9dbf-027c899d8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_coo_with_variable_blocks(attentions, block_positions, max_rows, max_cols):\n",
    "    # Listes pour stocker les données du tensor COO\n",
    "    indices_i = []\n",
    "    indices_j = []\n",
    "    values = []\n",
    "    \n",
    "    # Parcourir chaque bloc et ses indices positionnels\n",
    "    for (block, (block_i, block_j)) in zip(attentions, block_positions):\n",
    "        # Aplatir le bloc\n",
    "        flat_block = block.flatten()\n",
    "        \n",
    "        # Obtenir les indices relatifs à l'intérieur du bloc\n",
    "        num_rows, num_cols = block.shape\n",
    "        for idx, val in enumerate(flat_block):\n",
    "            # Calculer l'indice relatif dans le bloc\n",
    "            row_idx = idx // num_cols\n",
    "            col_idx = idx % num_cols\n",
    "            \n",
    "            # Calculer les indices globaux\n",
    "            global_row_idx = block_i + row_idx\n",
    "            global_col_idx = block_j + col_idx\n",
    "            \n",
    "            # Assurer que les indices sont dans les limites\n",
    "            if global_row_idx < max_rows and global_col_idx < max_cols:\n",
    "                indices_i.append(global_row_idx)\n",
    "                indices_j.append(global_col_idx)\n",
    "                values.append(val)\n",
    "    \n",
    "    indices = torch.LongTensor([indices_i, indices_j])\n",
    "    values = torch.FloatTensor(values)\n",
    "    size = (max_rows, max_cols)\n",
    "    \n",
    "    sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n",
    "    \n",
    "    return sparse_tensor\n",
    "\n",
    "# Exemple d'utilisation\n",
    "blocks = [torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6]])]\n",
    "positions = [(0, 0), (2, 1)]\n",
    "max_rows, max_cols = 10000, 100000  # Taille globale de la matrice\n",
    "\n",
    "sparse_matrix = create_sparse_coo_with_variable_blocks(blocks, positions, max_rows, max_cols)\n",
    "dense_matrix = sparse_matrix.to_dense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae6b94-e5f7-4ef4-82da-246e95d4413a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877393e-117f-481a-947a-ed3208f121e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01718c-5a64-4e5a-95e4-1cb83805139f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750885f2-b239-4a4c-9b59-d6786a5e7273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a2450-37f4-47af-bbef-ba5f8e6007d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
